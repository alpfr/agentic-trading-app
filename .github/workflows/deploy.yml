name: Build, Push & Deploy to EKS

on:
  push:
    branches: [ master ]
  workflow_dispatch:   # Allow manual trigger from GitHub UI

env:
  AWS_REGION:       us-east-1
  AWS_ACCOUNT_ID:   "713220200108"
  CLUSTER_NAME:     agentic-trading-cluster
  NAMESPACE:        agentic-trading-platform
  BACKEND_IMAGE:    agentic-trading-backend
  FRONTEND_IMAGE:   agentic-trading-frontend

jobs:
  # ============================================================
  # JOB 1 â€” Lint & basic checks
  # ============================================================
  lint:
    name: Lint & Syntax Check
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install backend deps
        run: |
          pip install -r backend/requirements.txt --quiet

      - name: Syntax check all Python files
        run: |
          python -m py_compile backend/app.py
          python -m py_compile backend/core/database.py
          python -m py_compile backend/core/risk_gatekeeper.py
          python -m py_compile backend/agents/market_data.py
          python -m py_compile backend/agents/strategy.py
          python -m py_compile backend/trading_interface/security/__init__.py
          python -m py_compile backend/trading_interface/execution/agent.py
          python -m py_compile backend/trading_interface/reconciliation/job.py
          echo "All Python files passed syntax check"

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "npm"
          cache-dependency-path: frontend/package-lock.json

      - name: Install frontend deps
        run: npm ci --prefix frontend

      - name: Lint frontend
        run: npm run lint --prefix frontend || true

  # ============================================================
  # JOB 2 â€” Build & Push Docker images to ECR
  # ============================================================
  build-and-push:
    name: Build & Push to ECR
    runs-on: ubuntu-latest
    needs: lint
    outputs:
      backend-image:  ${{ steps.meta-backend.outputs.image }}
      frontend-image: ${{ steps.meta-frontend.outputs.image }}
      image-tag:      ${{ steps.tag.outputs.tag }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Generate image tag
        id: tag
        run: |
          TAG="${GITHUB_SHA::8}"
          echo "tag=$TAG" >> $GITHUB_OUTPUT
          echo "Image tag: $TAG"

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id:     ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region:            ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: ecr-login
        uses: aws-actions/amazon-ecr-login@v2

      - name: Create ECR repositories if they don't exist
        run: |
          for REPO in $BACKEND_IMAGE $FRONTEND_IMAGE; do
            aws ecr describe-repositories --repository-names "$REPO" --region "$AWS_REGION" 2>/dev/null || \
            aws ecr create-repository \
              --repository-name "$REPO" \
              --region "$AWS_REGION" \
              --image-scanning-configuration scanOnPush=true \
              --encryption-configuration encryptionType=AES256
            echo "ECR repo ready: $REPO"
          done

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      # â”€â”€ BACKEND â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Build & push backend
        id: meta-backend
        uses: docker/build-push-action@v5
        with:
          context:   ./backend
          push:      true
          tags: |
            ${{ env.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/${{ env.BACKEND_IMAGE }}:${{ steps.tag.outputs.tag }}
            ${{ env.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/${{ env.BACKEND_IMAGE }}:latest
          cache-from: type=gha
          cache-to:   type=gha,mode=max

      # â”€â”€ FRONTEND â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Write frontend env for production build
        run: |
          # Leave VITE_API_BASE_URL unset so the frontend falls back to
          # window.location.origin at runtime â€” ALB routes /api/* to backend.
          cat > frontend/.env.production << ENVEOF
          VITE_API_KEY=${{ secrets.APP_API_KEY }}
          ENVEOF

      - name: Build & push frontend
        id: meta-frontend
        uses: docker/build-push-action@v5
        with:
          context:   ./frontend
          push:      true
          tags: |
            ${{ env.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/${{ env.FRONTEND_IMAGE }}:${{ steps.tag.outputs.tag }}
            ${{ env.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/${{ env.FRONTEND_IMAGE }}:latest
          cache-from: type=gha
          cache-to:   type=gha,mode=max

      - name: Summary
        run: |
          echo "### Docker Images Pushed ðŸ³" >> $GITHUB_STEP_SUMMARY
          echo "| Image | Tag |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|-----|" >> $GITHUB_STEP_SUMMARY
          echo "| $BACKEND_IMAGE | ${{ steps.tag.outputs.tag }} |" >> $GITHUB_STEP_SUMMARY
          echo "| $FRONTEND_IMAGE | ${{ steps.tag.outputs.tag }} |" >> $GITHUB_STEP_SUMMARY

  # ============================================================
  # JOB 3 â€” Provision EKS cluster + ALB controller (idempotent)
  # ============================================================
  provision-cluster:
    name: Provision EKS Cluster
    runs-on: ubuntu-latest
    needs: build-and-push

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id:     ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region:            ${{ env.AWS_REGION }}

      - name: Install eksctl
        run: |
          EKSCTL_VERSION=$(curl -sL https://api.github.com/repos/eksctl-io/eksctl/releases/latest \
            | grep '"tag_name"' | cut -d'"' -f4)
          curl -sL "https://github.com/eksctl-io/eksctl/releases/download/${EKSCTL_VERSION}/eksctl_Linux_amd64.tar.gz" \
            | tar xz -C /tmp
          sudo mv /tmp/eksctl /usr/local/bin/
          eksctl version

      - name: Install kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: "v1.29.0"

      - name: Install Helm
        uses: azure/setup-helm@v3
        with:
          version: "v3.14.0"

      - name: Check if cluster exists
        id: cluster-check
        run: |
          if aws eks describe-cluster --name "$CLUSTER_NAME" --region "$AWS_REGION" &>/dev/null; then
            echo "exists=true" >> $GITHUB_OUTPUT
            echo "Cluster already exists â€” skipping creation"
          else
            echo "exists=false" >> $GITHUB_OUTPUT
            echo "Cluster does not exist â€” will create"
          fi

      - name: Create EKS cluster
        if: steps.cluster-check.outputs.exists == 'false'
        run: |
          cat > /tmp/cluster-config.yaml << CLUSTEREOF
          apiVersion: eksctl.io/v1alpha5
          kind: ClusterConfig
          metadata:
            name: ${CLUSTER_NAME}
            region: ${AWS_REGION}
            version: "1.29"
          iam:
            withOIDC: true
          managedNodeGroups:
            - name: agentic-trading-nodes
              instanceType: t3.medium
              desiredCapacity: 2
              minSize: 1
              maxSize: 4
              amiFamily: AmazonLinux2
              privateNetworking: true
              tags:
                app: agentic-trading
                env: production
              iam:
                attachPolicyARNs:
                  - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
                  - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
                  - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
                  - arn:aws:iam::aws:policy/ElasticLoadBalancingFullAccess
          addons:
            - name: vpc-cni
              resolveConflicts: overwrite
            - name: coredns
              resolveConflicts: overwrite
            - name: kube-proxy
              resolveConflicts: overwrite
          CLUSTEREOF
          eksctl create cluster -f /tmp/cluster-config.yaml
          echo "Cluster created successfully"

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --name "$CLUSTER_NAME" --region "$AWS_REGION"
          kubectl cluster-info

      - name: Create namespace
        run: |
          kubectl create namespace "$NAMESPACE" --dry-run=client -o yaml | kubectl apply -f -
          kubectl label namespace "$NAMESPACE" app=agentic-trading --overwrite
          echo "Namespace '$NAMESPACE' ready"

      - name: Install ALB controller IAM policy
        run: |
          ALB_POLICY_NAME="AWSLoadBalancerControllerIAMPolicy"
          if aws iam get-policy \
              --policy-arn "arn:aws:iam::${AWS_ACCOUNT_ID}:policy/${ALB_POLICY_NAME}" &>/dev/null; then
            echo "ALB IAM policy already exists"
          else
            curl -sL https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.7.1/docs/install/iam_policy.json \
              -o /tmp/alb-iam-policy.json
            aws iam create-policy \
              --policy-name "$ALB_POLICY_NAME" \
              --policy-document file:///tmp/alb-iam-policy.json
            echo "ALB IAM policy created"
          fi

      - name: Create ALB controller IAM service account
        run: |
          eksctl create iamserviceaccount \
            --cluster="$CLUSTER_NAME" \
            --namespace=kube-system \
            --name=aws-load-balancer-controller \
            --attach-policy-arn="arn:aws:iam::${AWS_ACCOUNT_ID}:policy/AWSLoadBalancerControllerIAMPolicy" \
            --override-existing-serviceaccounts \
            --approve \
            --region "$AWS_REGION" || echo "Service account may already exist â€” continuing"

      - name: Install ALB controller via Helm
        run: |
          helm repo add eks https://aws.github.io/eks-charts --force-update
          helm repo update
          VPC_ID=$(aws eks describe-cluster \
            --name "$CLUSTER_NAME" \
            --query 'cluster.resourcesVpcConfig.vpcId' \
            --output text)
          helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller \
            -n kube-system \
            --set clusterName="$CLUSTER_NAME" \
            --set serviceAccount.create=false \
            --set serviceAccount.name=aws-load-balancer-controller \
            --set region="$AWS_REGION" \
            --set vpcId="$VPC_ID" \
            --wait --timeout 5m
          echo "ALB controller installed"

      - name: Create Kubernetes secrets
        run: |
          kubectl create secret generic trading-app-secrets \
            --namespace="$NAMESPACE" \
            --from-literal=openai-api-key="${{ secrets.OPENAI_API_KEY }}" \
            --from-literal=alpaca-api-key="${{ secrets.ALPACA_API_KEY }}" \
            --from-literal=alpaca-secret-key="${{ secrets.ALPACA_SECRET_KEY }}" \
            --from-literal=app-api-key="${{ secrets.APP_API_KEY }}" \
            --from-literal=jwt-secret="${{ secrets.JWT_SECRET }}" \
            --from-literal=admin-username="${{ secrets.ADMIN_USERNAME }}" \
            --from-literal=admin-password-hash="${{ secrets.ADMIN_PASSWORD_HASH }}" \
            --from-literal=admin-totp-secret="${{ secrets.ADMIN_TOTP_SECRET }}" \
            --from-literal=cors-allowed-origins="${{ secrets.CORS_ALLOWED_ORIGINS || 'https://agentictradepulse.opssightai.com' }}" \
            --from-literal=database-url="${{ secrets.DATABASE_URL }}" \
            --dry-run=client -o yaml | kubectl apply -f -
          echo "K8s secrets applied"

  # ============================================================
  # JOB 4 â€” Deploy app to EKS
  # ============================================================
  deploy:
    name: Deploy to EKS
    runs-on: ubuntu-latest
    needs: [ build-and-push, provision-cluster ]
    environment: production

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id:     ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region:            ${{ env.AWS_REGION }}

      - name: Install kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: "v1.29.0"

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --name "$CLUSTER_NAME" --region "$AWS_REGION"

      - name: Resolve ACM certificate ARN
        id: acm
        run: |
          # Auto-resolve the ACM cert ARN for opssightai.com
          # Override by setting ACM_CERT_ARN in GitHub Secrets.
          if [ -n "${{ secrets.ACM_CERT_ARN }}" ]; then
            echo "cert_arn=${{ secrets.ACM_CERT_ARN }}" >> $GITHUB_OUTPUT
            echo "Using manually set ACM_CERT_ARN secret"
          else
            # Step 1: exact wildcard primary domain
            CERT_ARN=$(aws acm list-certificates \
              --region "$AWS_REGION" \
              --query "CertificateSummaryList[?DomainName=='*.opssightai.com' && Status=='ISSUED'].CertificateArn" \
              --output text | awk '{print $1}')

            # Step 2: wildcard as a SAN (primary domain is opssightai.com, SAN is *.opssightai.com)
            if [ -z "$CERT_ARN" ]; then
              ALL_ARNS=$(aws acm list-certificates \
                --region "$AWS_REGION" \
                --query "CertificateSummaryList[?Status=='ISSUED'].CertificateArn" \
                --output text)
              for ARN in $ALL_ARNS; do
                COVERS=$(aws acm describe-certificate \
                  --certificate-arn "$ARN" \
                  --region "$AWS_REGION" \
                  --query "Certificate.SubjectAlternativeNames" \
                  --output text)
                if echo "$COVERS" | grep -q "\*.opssightai.com"; then
                  CERT_ARN="$ARN"
                  echo "Found wildcard as SAN in cert: $ARN"
                  break
                fi
              done
            fi

            # Step 3: any issued cert covering the root domain
            if [ -z "$CERT_ARN" ]; then
              CERT_ARN=$(aws acm list-certificates \
                --region "$AWS_REGION" \
                --query "CertificateSummaryList[?contains(DomainName,'opssightai.com') && Status=='ISSUED'].CertificateArn" \
                --output text | awk '{print $1}')
            fi
            if [ -z "$CERT_ARN" ]; then
              echo "::error::No issued ACM certificate found for opssightai.com. Set ACM_CERT_ARN in GitHub Secrets."
              exit 1
            fi
            echo "cert_arn=$CERT_ARN" >> $GITHUB_OUTPUT
            echo "Resolved ACM cert: $CERT_ARN"
          fi

      - name: Generate deployment manifest
        env:
          ACM_CERT_ARN: ${{ steps.acm.outputs.cert_arn }}
        run: |
          ECR_BASE="${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com"
          IMAGE_TAG="${{ needs.build-and-push.outputs.image-tag }}"

          cat > /tmp/k8s-deploy.yaml << MANIFEST
          ---
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: agentic-trading-backend
            namespace: ${NAMESPACE}
            labels:
              app: agentic-trading-backend
              version: "${IMAGE_TAG}"
          spec:
            replicas: 2
            selector:
              matchLabels:
                app: agentic-trading-backend
            strategy:
              type: RollingUpdate
              rollingUpdate:
                maxSurge: 1
                maxUnavailable: 0
            template:
              metadata:
                labels:
                  app: agentic-trading-backend
                  version: "${IMAGE_TAG}"
              spec:
                containers:
                - name: backend
                  image: ${ECR_BASE}/${BACKEND_IMAGE}:${IMAGE_TAG}
                  imagePullPolicy: Always
                  ports:
                  - containerPort: 8000
                  resources:
                    requests:
                      cpu: "250m"
                      memory: "512Mi"
                    limits:
                      cpu: "1000m"
                      memory: "1Gi"
                  env:
                  - name: CORS_ALLOWED_ORIGINS
                    value: "https://agentictradepulse.opssightai.com"
                  - name: OPENAI_API_KEY
                    valueFrom:
                      secretKeyRef:
                        name: trading-app-secrets
                        key: openai-api-key
                  - name: ALPACA_API_KEY
                    valueFrom:
                      secretKeyRef:
                        name: trading-app-secrets
                        key: alpaca-api-key
                  - name: ALPACA_SECRET_KEY
                    valueFrom:
                      secretKeyRef:
                        name: trading-app-secrets
                        key: alpaca-secret-key
                  - name: APP_API_KEY
                    valueFrom:
                      secretKeyRef:
                        name: trading-app-secrets
                        key: app-api-key
                  - name: DATABASE_URL
                    valueFrom:
                      secretKeyRef:
                        name: trading-app-secrets
                        key: database-url
                  livenessProbe:
                    httpGet:
                      path: /health
                      port: 8000
                    initialDelaySeconds: 15
                    periodSeconds: 20
                    failureThreshold: 3
                  readinessProbe:
                    httpGet:
                      path: /health
                      port: 8000
                    initialDelaySeconds: 5
                    periodSeconds: 10
                    failureThreshold: 3
          ---
          apiVersion: v1
          kind: Service
          metadata:
            name: agentic-trading-backend
            namespace: ${NAMESPACE}
          spec:
            selector:
              app: agentic-trading-backend
            ports:
            - protocol: TCP
              port: 8000
              targetPort: 8000
            type: ClusterIP
          ---
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: agentic-trading-frontend
            namespace: ${NAMESPACE}
            labels:
              app: agentic-trading-frontend
              version: "${IMAGE_TAG}"
          spec:
            replicas: 2
            selector:
              matchLabels:
                app: agentic-trading-frontend
            strategy:
              type: RollingUpdate
              rollingUpdate:
                maxSurge: 1
                maxUnavailable: 0
            template:
              metadata:
                labels:
                  app: agentic-trading-frontend
                  version: "${IMAGE_TAG}"
              spec:
                containers:
                - name: frontend
                  image: ${ECR_BASE}/${FRONTEND_IMAGE}:${IMAGE_TAG}
                  imagePullPolicy: Always
                  ports:
                  - containerPort: 80
                  resources:
                    requests:
                      cpu: "100m"
                      memory: "128Mi"
                    limits:
                      cpu: "250m"
                      memory: "256Mi"
                  livenessProbe:
                    httpGet:
                      path: /
                      port: 80
                    initialDelaySeconds: 10
                    periodSeconds: 15
                  readinessProbe:
                    httpGet:
                      path: /
                      port: 80
                    initialDelaySeconds: 5
                    periodSeconds: 10
          ---
          apiVersion: v1
          kind: Service
          metadata:
            name: agentic-trading-frontend
            namespace: ${NAMESPACE}
          spec:
            selector:
              app: agentic-trading-frontend
            ports:
            - protocol: TCP
              port: 80
              targetPort: 80
            type: ClusterIP
          ---
          apiVersion: networking.k8s.io/v1
          kind: Ingress
          metadata:
            name: agentic-trading-ingress
            namespace: ${NAMESPACE}
            annotations:
              kubernetes.io/ingress.class: alb
              alb.ingress.kubernetes.io/scheme: internet-facing
              alb.ingress.kubernetes.io/target-type: ip
              alb.ingress.kubernetes.io/healthcheck-path: /health
              alb.ingress.kubernetes.io/healthcheck-interval-seconds: "15"
              alb.ingress.kubernetes.io/success-codes: "200"
              alb.ingress.kubernetes.io/load-balancer-name: agentic-trading-alb
              alb.ingress.kubernetes.io/certificate-arn: ${ACM_CERT_ARN}
              alb.ingress.kubernetes.io/listen-ports: '[{"HTTP":80},{"HTTPS":443}]'
              alb.ingress.kubernetes.io/ssl-redirect: '443'
              alb.ingress.kubernetes.io/ssl-policy: ELBSecurityPolicy-TLS13-1-2-2021-06
              alb.ingress.kubernetes.io/load-balancer-attributes: routing.http.drop_invalid_header_fields.enabled=true
          spec:
            rules:
            - host: agentictradepulse.opssightai.com
              http:
                paths:
                - path: /api
                  pathType: Prefix
                  backend:
                    service:
                      name: agentic-trading-backend
                      port:
                        number: 8000
                - path: /health
                  pathType: Prefix
                  backend:
                    service:
                      name: agentic-trading-backend
                      port:
                        number: 8000
                - path: /
                  pathType: Prefix
                  backend:
                    service:
                      name: agentic-trading-frontend
                      port:
                        number: 80
          MANIFEST
          echo "Manifest generated for image tag: $IMAGE_TAG"

      - name: Apply deployment
        run: |
          kubectl apply -f /tmp/k8s-deploy.yaml
          echo "Manifests applied"

      - name: Wait for backend rollout
        run: |
          kubectl rollout status deployment/agentic-trading-backend \
            -n "$NAMESPACE" --timeout=300s

      - name: Wait for frontend rollout
        run: |
          kubectl rollout status deployment/agentic-trading-frontend \
            -n "$NAMESPACE" --timeout=300s

      - name: Wait for ALB and get URL
        id: alb
        run: |
          echo "Waiting for ALB to be provisioned..."
          for i in $(seq 1 40); do
            ALB_URL=$(kubectl get ingress agentic-trading-ingress \
              -n "$NAMESPACE" \
              -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
            if [[ -n "$ALB_URL" ]]; then
              echo "alb_url=$ALB_URL" >> $GITHUB_OUTPUT
              echo "ALB ready: $ALB_URL"
              break
            fi
            echo "Attempt $i/40 â€” waiting 15s..."
            sleep 15
          done

      - name: Health check
        run: |
          ALB_URL="${{ steps.alb.outputs.alb_url }}"
          if [[ -z "$ALB_URL" ]]; then
            echo "ALB URL not yet available â€” check ingress manually"
            exit 0
          fi
          echo "Running health check against http://$ALB_URL/health ..."
          sleep 20
          for i in $(seq 1 10); do
            STATUS=$(curl -s -o /dev/null -w "%{http_code}" \
              --max-time 10 "http://$ALB_URL/health" || echo "000")
            if [[ "$STATUS" == "200" ]]; then
              echo "Health check PASSED (HTTP 200)"
              break
            fi
            echo "Attempt $i â€” HTTP $STATUS, retrying in 15s..."
            sleep 15
          done

      - name: Print deployment summary
        run: |
          ALB_URL="${{ steps.alb.outputs.alb_url }}"
          IMAGE_TAG="${{ needs.build-and-push.outputs.image-tag }}"
          echo ""
          echo "### Deployment Complete âœ…" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Field | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Cluster | $CLUSTER_NAME |" >> $GITHUB_STEP_SUMMARY
          echo "| Namespace | $NAMESPACE |" >> $GITHUB_STEP_SUMMARY
          echo "| Image Tag | $IMAGE_TAG |" >> $GITHUB_STEP_SUMMARY
          echo "| App URL | http://$ALB_URL |" >> $GITHUB_STEP_SUMMARY
          echo "| Health | http://$ALB_URL/health |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          kubectl get pods -n "$NAMESPACE" >> $GITHUB_STEP_SUMMARY || true
          echo ""
          echo "App URL:    http://$ALB_URL"
          echo "Health URL: http://$ALB_URL/health"
          kubectl get pods -n "$NAMESPACE"
          kubectl get ingress -n "$NAMESPACE"
