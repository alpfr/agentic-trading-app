name: Build, Push & Deploy to EKS

on:
  push:
    branches: [ master ]
  workflow_dispatch:   # Allow manual trigger from GitHub UI

env:
  AWS_REGION:       us-east-1
  AWS_ACCOUNT_ID:   "713220200108"
  CLUSTER_NAME:     agentic-trading-cluster
  NAMESPACE:        agentic-trading-platform
  BACKEND_IMAGE:    agentic-trading-backend
  FRONTEND_IMAGE:   agentic-trading-frontend

jobs:
  # ============================================================
  # JOB 1 â€” Lint & basic checks
  # ============================================================
  lint:
    name: Lint & Syntax Check
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install backend deps
        run: |
          pip install -r backend/requirements.txt --quiet

      - name: Syntax check all Python files
        run: |
          python -m py_compile backend/app.py
          python -m py_compile backend/core/database.py
          python -m py_compile backend/core/risk_gatekeeper.py
          python -m py_compile backend/agents/market_data.py
          python -m py_compile backend/agents/strategy.py
          python -m py_compile backend/trading_interface/security/__init__.py
          python -m py_compile backend/trading_interface/execution/agent.py
          python -m py_compile backend/trading_interface/reconciliation/job.py
          echo "All Python files passed syntax check"

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "npm"
          cache-dependency-path: frontend/package-lock.json

      - name: Install frontend deps
        run: npm ci --prefix frontend

      - name: Lint frontend
        run: npm run lint --prefix frontend || true

  # ============================================================
  # JOB 2 â€” Build & Push Docker images to ECR
  # ============================================================
  build-and-push:
    name: Build & Push to ECR
    runs-on: ubuntu-latest
    needs: lint
    outputs:
      backend-image:  ${{ steps.meta-backend.outputs.image }}
      frontend-image: ${{ steps.meta-frontend.outputs.image }}
      image-tag:      ${{ steps.tag.outputs.tag }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Generate image tag
        id: tag
        run: |
          TAG="${GITHUB_SHA::8}"
          echo "tag=$TAG" >> $GITHUB_OUTPUT
          echo "Image tag: $TAG"

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id:     ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region:            ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: ecr-login
        uses: aws-actions/amazon-ecr-login@v2

      - name: Create ECR repositories if they don't exist
        run: |
          for REPO in $BACKEND_IMAGE $FRONTEND_IMAGE; do
            aws ecr describe-repositories --repository-names "$REPO" --region "$AWS_REGION" 2>/dev/null || \
            aws ecr create-repository \
              --repository-name "$REPO" \
              --region "$AWS_REGION" \
              --image-scanning-configuration scanOnPush=true \
              --encryption-configuration encryptionType=AES256
            echo "ECR repo ready: $REPO"
          done

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      # â”€â”€ BACKEND â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Build & push backend
        id: meta-backend
        uses: docker/build-push-action@v5
        with:
          context:   ./backend
          push:      true
          tags: |
            ${{ env.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/${{ env.BACKEND_IMAGE }}:${{ steps.tag.outputs.tag }}
            ${{ env.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/${{ env.BACKEND_IMAGE }}:latest
          cache-from: type=gha
          cache-to:   type=gha,mode=max

      # â”€â”€ FRONTEND â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Write frontend env for production build
        run: |
          # Leave VITE_API_BASE_URL unset so the frontend falls back to
          # window.location.origin at runtime â€” ALB routes /api/* to backend.
          cat > frontend/.env.production << ENVEOF
          VITE_API_KEY=${{ secrets.APP_API_KEY }}
          ENVEOF

      - name: Build & push frontend
        id: meta-frontend
        uses: docker/build-push-action@v5
        with:
          context:   ./frontend
          push:      true
          tags: |
            ${{ env.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/${{ env.FRONTEND_IMAGE }}:${{ steps.tag.outputs.tag }}
            ${{ env.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/${{ env.FRONTEND_IMAGE }}:latest
          cache-from: type=gha
          cache-to:   type=gha,mode=max

      - name: Summary
        run: |
          echo "### Docker Images Pushed ðŸ³" >> $GITHUB_STEP_SUMMARY
          echo "| Image | Tag |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|-----|" >> $GITHUB_STEP_SUMMARY
          echo "| $BACKEND_IMAGE | ${{ steps.tag.outputs.tag }} |" >> $GITHUB_STEP_SUMMARY
          echo "| $FRONTEND_IMAGE | ${{ steps.tag.outputs.tag }} |" >> $GITHUB_STEP_SUMMARY

  # ============================================================
  # JOB 3 â€” Provision EKS cluster + ALB controller (idempotent)
  # ============================================================
  provision-cluster:
    name: Provision EKS Cluster
    runs-on: ubuntu-latest
    needs: build-and-push
    outputs:
      waf_arn: ${{ steps.waf.outputs.waf_arn }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id:     ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region:            ${{ env.AWS_REGION }}

      - name: Install eksctl
        run: |
          EKSCTL_VERSION=$(curl -sL https://api.github.com/repos/eksctl-io/eksctl/releases/latest \
            | grep '"tag_name"' | cut -d'"' -f4)
          curl -sL "https://github.com/eksctl-io/eksctl/releases/download/${EKSCTL_VERSION}/eksctl_Linux_amd64.tar.gz" \
            | tar xz -C /tmp
          sudo mv /tmp/eksctl /usr/local/bin/
          eksctl version

      - name: Install kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: "v1.29.0"

      - name: Install Helm
        uses: azure/setup-helm@v3
        with:
          version: "v3.14.0"

      - name: Check if cluster exists
        id: cluster-check
        run: |
          if aws eks describe-cluster --name "$CLUSTER_NAME" --region "$AWS_REGION" &>/dev/null; then
            echo "exists=true" >> $GITHUB_OUTPUT
            echo "Cluster already exists â€” skipping creation"
          else
            echo "exists=false" >> $GITHUB_OUTPUT
            echo "Cluster does not exist â€” will create"
          fi

      - name: Create EKS cluster
        if: steps.cluster-check.outputs.exists == 'false'
        run: |
          cat > /tmp/cluster-config.yaml << CLUSTEREOF
          apiVersion: eksctl.io/v1alpha5
          kind: ClusterConfig
          metadata:
            name: ${CLUSTER_NAME}
            region: ${AWS_REGION}
            version: "1.29"
          iam:
            withOIDC: true
          managedNodeGroups:
            - name: agentic-trading-nodes
              instanceType: t3.medium
              desiredCapacity: 2
              minSize: 1
              maxSize: 4
              amiFamily: AmazonLinux2
              privateNetworking: true
              tags:
                app: agentic-trading
                env: production
              iam:
                attachPolicyARNs:
                  - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
                  - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
                  - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
                  - arn:aws:iam::aws:policy/ElasticLoadBalancingFullAccess
          addons:
            - name: vpc-cni
              resolveConflicts: overwrite
            - name: coredns
              resolveConflicts: overwrite
            - name: kube-proxy
              resolveConflicts: overwrite
          CLUSTEREOF
          eksctl create cluster -f /tmp/cluster-config.yaml
          echo "Cluster created successfully"

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --name "$CLUSTER_NAME" --region "$AWS_REGION"
          kubectl cluster-info

      - name: Create namespace
        run: |
          kubectl create namespace "$NAMESPACE" --dry-run=client -o yaml | kubectl apply -f -
          kubectl label namespace "$NAMESPACE" app=agentic-trading --overwrite
          echo "Namespace '$NAMESPACE' ready"

      - name: Install ALB controller IAM policy
        run: |
          ALB_POLICY_NAME="AWSLoadBalancerControllerIAMPolicy"
          if aws iam get-policy \
              --policy-arn "arn:aws:iam::${AWS_ACCOUNT_ID}:policy/${ALB_POLICY_NAME}" &>/dev/null; then
            echo "ALB IAM policy already exists"
          else
            curl -sL https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.7.1/docs/install/iam_policy.json \
              -o /tmp/alb-iam-policy.json
            aws iam create-policy \
              --policy-name "$ALB_POLICY_NAME" \
              --policy-document file:///tmp/alb-iam-policy.json
            echo "ALB IAM policy created"
          fi

      - name: Create ALB controller IAM service account
        run: |
          eksctl create iamserviceaccount \
            --cluster="$CLUSTER_NAME" \
            --namespace=kube-system \
            --name=aws-load-balancer-controller \
            --attach-policy-arn="arn:aws:iam::${AWS_ACCOUNT_ID}:policy/AWSLoadBalancerControllerIAMPolicy" \
            --override-existing-serviceaccounts \
            --approve \
            --region "$AWS_REGION" || echo "Service account may already exist â€” continuing"

      - name: Install ALB controller via Helm
        run: |
          helm repo add eks https://aws.github.io/eks-charts --force-update
          helm repo update
          VPC_ID=$(aws eks describe-cluster \
            --name "$CLUSTER_NAME" \
            --query 'cluster.resourcesVpcConfig.vpcId' \
            --output text)
          helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller \
            -n kube-system \
            --set clusterName="$CLUSTER_NAME" \
            --set serviceAccount.create=false \
            --set serviceAccount.name=aws-load-balancer-controller \
            --set region="$AWS_REGION" \
            --set vpcId="$VPC_ID" \
            --wait --timeout 5m
          echo "ALB controller installed"

      - name: Create Kubernetes secrets
        run: |
          kubectl create secret generic trading-app-secrets \
            --namespace="$NAMESPACE" \
            --from-literal=openai-api-key="${{ secrets.OPENAI_API_KEY }}" \
            --from-literal=alpaca-api-key="${{ secrets.ALPACA_API_KEY }}" \
            --from-literal=alpaca-secret-key="${{ secrets.ALPACA_SECRET_KEY }}" \
            --from-literal=app-api-key="${{ secrets.APP_API_KEY }}" \
            --from-literal=jwt-secret="${{ secrets.JWT_SECRET }}" \
            --from-literal=admin-username="${{ secrets.ADMIN_USERNAME }}" \
            --from-literal=admin-password-hash="${{ secrets.ADMIN_PASSWORD_HASH }}" \
            --from-literal=admin-totp-secret="${{ secrets.ADMIN_TOTP_SECRET }}" \
            --from-literal=cors-allowed-origins="${{ secrets.CORS_ALLOWED_ORIGINS || 'https://agentictradepulse.opssightai.com' }}" \
            --from-literal=database-url="${{ secrets.DATABASE_URL || 'sqlite:////app/trading.db' }}"            --dry-run=client -o yaml | kubectl apply -f -
          echo "K8s secrets applied"

      - name: Set up CloudWatch log group + retention + Fluent Bit
        run: |
          LOG_GROUP="/agentic-trading/eks/application"
          RETENTION_DAYS=90

          # â”€â”€ 1. Create log group (idempotent) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          aws logs create-log-group             --log-group-name "$LOG_GROUP"             --region "$AWS_REGION" 2>/dev/null || echo "Log group already exists"

          # â”€â”€ 2. Set 90-day retention policy â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          aws logs put-retention-policy             --log-group-name "$LOG_GROUP"             --retention-in-days $RETENTION_DAYS             --region "$AWS_REGION"
          echo "âœ… Log group: $LOG_GROUP (${RETENTION_DAYS}d retention)"

          # â”€â”€ 3. Tag for cost tracking â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          aws logs tag-log-group             --log-group-name "$LOG_GROUP"             --tags app=agentic-trading,env=production,soc2=true             --region "$AWS_REGION"

          # â”€â”€ 4. IAM policy for Fluent Bit â†’ CloudWatch â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          POLICY_NAME="FluentBitCloudWatchPolicy"
          POLICY_DOC='{
            "Version": "2012-10-17",
            "Statement": [{
              "Effect": "Allow",
              "Action": [
                "logs:CreateLogGroup",
                "logs:CreateLogStream",
                "logs:PutLogEvents",
                "logs:DescribeLogGroups",
                "logs:DescribeLogStreams"
              ],
              "Resource": "arn:aws:logs:*:*:log-group:/agentic-trading/*"
            }]
          }'

          if aws iam get-policy               --policy-arn "arn:aws:iam::${AWS_ACCOUNT_ID}:policy/${POLICY_NAME}" &>/dev/null; then
            echo "FluentBit IAM policy already exists"
          else
            aws iam create-policy               --policy-name "$POLICY_NAME"               --policy-document "$POLICY_DOC"
            echo "âœ… FluentBit IAM policy created"
          fi

          # â”€â”€ 5. IRSA for Fluent Bit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          eksctl create iamserviceaccount             --cluster="$CLUSTER_NAME"             --namespace=amazon-cloudwatch             --name=fluent-bit             --attach-policy-arn="arn:aws:iam::${AWS_ACCOUNT_ID}:policy/${POLICY_NAME}"             --override-existing-serviceaccounts             --approve             --region "$AWS_REGION" || echo "Fluent Bit service account may already exist"

          # â”€â”€ 6. Deploy Fluent Bit DaemonSet via AWS addon â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          kubectl create namespace amazon-cloudwatch --dry-run=client -o yaml | kubectl apply -f -

          # ConfigMap for Fluent Bit
          kubectl apply -f - <<FLUENTEOF
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: fluent-bit-config
            namespace: amazon-cloudwatch
          data:
            fluent-bit.conf: |
              [SERVICE]
                  Flush         5
                  Log_Level     info
                  Parsers_File  parsers.conf

              [INPUT]
                  Name              tail
                  Tag               kube.*
                  Path              /var/log/containers/agentic-trading-backend*.log
                  Parser            docker
                  DB                /var/log/flb_kube.db
                  Mem_Buf_Limit     5MB
                  Skip_Long_Lines   On
                  Refresh_Interval  10

              [FILTER]
                  Name                kubernetes
                  Match               kube.*
                  Kube_URL            https://kubernetes.default.svc:443
                  Kube_CA_File        /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
                  Kube_Token_File     /var/run/secrets/kubernetes.io/serviceaccount/token
                  Merge_Log           On
                  Keep_Log            Off

              [OUTPUT]
                  Name                cloudwatch_logs
                  Match               kube.*
                  region              ${AWS_REGION}
                  log_group_name      /agentic-trading/eks/application
                  log_stream_prefix   backend-
                  auto_create_group   false

            parsers.conf: |
              [PARSER]
                  Name        docker
                  Format      json
                  Time_Key    time
                  Time_Format %Y-%m-%dT%H:%M:%S.%LZ
          FLUENTEOF

          # Deploy Fluent Bit DaemonSet
          kubectl apply -f - <<DSEOF
          apiVersion: apps/v1
          kind: DaemonSet
          metadata:
            name: fluent-bit
            namespace: amazon-cloudwatch
            labels:
              app: fluent-bit
          spec:
            selector:
              matchLabels:
                app: fluent-bit
            template:
              metadata:
                labels:
                  app: fluent-bit
              spec:
                serviceAccountName: fluent-bit
                tolerations:
                  - key: node-role.kubernetes.io/master
                    effect: NoSchedule
                containers:
                  - name: fluent-bit
                    image: public.ecr.aws/aws-observability/aws-for-fluent-bit:stable
                    resources:
                      limits:
                        memory: 200Mi
                      requests:
                        cpu: 50m
                        memory: 100Mi
                    volumeMounts:
                      - name: varlog
                        mountPath: /var/log
                      - name: varlibdockercontainers
                        mountPath: /var/lib/docker/containers
                        readOnly: true
                      - name: fluent-bit-config
                        mountPath: /fluent-bit/etc/
                volumes:
                  - name: varlog
                    hostPath:
                      path: /var/log
                  - name: varlibdockercontainers
                    hostPath:
                      path: /var/lib/docker/containers
                  - name: fluent-bit-config
                    configMap:
                      name: fluent-bit-config
          DSEOF
          echo "âœ… Fluent Bit DaemonSet deployed"


      - name: Create WAF Web ACL and attach to ALB
        id: waf
        run: |
          REGION="$AWS_REGION"
          WAF_NAME="agentic-trading-waf"

          # â”€â”€ 1. Create Web ACL (idempotent) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          EXISTING_ARN=$(aws wafv2 list-web-acls             --scope REGIONAL             --region "$REGION"             --query "WebACLs[?Name=='${WAF_NAME}'].ARN"             --output text 2>/dev/null)

          if [ -z "$EXISTING_ARN" ] || [ "$EXISTING_ARN" = "None" ]; then
            echo "Creating WAF Web ACL..."
            WAF_RESULT=$(aws wafv2 create-web-acl               --name "$WAF_NAME"               --scope REGIONAL               --region "$REGION"               --default-action Allow={}               --visibility-config SampledRequestsEnabled=true,CloudWatchMetricsEnabled=true,MetricName=AgenticTradingWAF               --rules '
              [
                {
                  "Name": "AWSManagedRulesCommonRuleSet",
                  "Priority": 1,
                  "OverrideAction": {"None": {}},
                  "Statement": {
                    "ManagedRuleGroupStatement": {
                      "VendorName": "AWS",
                      "Name": "AWSManagedRulesCommonRuleSet"
                    }
                  },
                  "VisibilityConfig": {
                    "SampledRequestsEnabled": true,
                    "CloudWatchMetricsEnabled": true,
                    "MetricName": "CommonRuleSet"
                  }
                },
                {
                  "Name": "AWSManagedRulesKnownBadInputsRuleSet",
                  "Priority": 2,
                  "OverrideAction": {"None": {}},
                  "Statement": {
                    "ManagedRuleGroupStatement": {
                      "VendorName": "AWS",
                      "Name": "AWSManagedRulesKnownBadInputsRuleSet"
                    }
                  },
                  "VisibilityConfig": {
                    "SampledRequestsEnabled": true,
                    "CloudWatchMetricsEnabled": true,
                    "MetricName": "KnownBadInputs"
                  }
                },
                {
                  "Name": "AWSManagedRulesSQLiRuleSet",
                  "Priority": 3,
                  "OverrideAction": {"None": {}},
                  "Statement": {
                    "ManagedRuleGroupStatement": {
                      "VendorName": "AWS",
                      "Name": "AWSManagedRulesSQLiRuleSet"
                    }
                  },
                  "VisibilityConfig": {
                    "SampledRequestsEnabled": true,
                    "CloudWatchMetricsEnabled": true,
                    "MetricName": "SQLiRuleSet"
                  }
                },
                {
                  "Name": "RateLimitRule",
                  "Priority": 4,
                  "Action": {"Block": {}},
                  "Statement": {
                    "RateBasedStatement": {
                      "Limit": 2000,
                      "AggregateKeyType": "IP"
                    }
                  },
                  "VisibilityConfig": {
                    "SampledRequestsEnabled": true,
                    "CloudWatchMetricsEnabled": true,
                    "MetricName": "RateLimit"
                  }
                }
              ]'               --output json)

            WAF_ARN=$(echo "$WAF_RESULT" | python3 -c "import json,sys; print(json.load(sys.stdin)['Summary']['ARN'])")
            echo "âœ… WAF Web ACL created: $WAF_ARN"
          else
            WAF_ARN="$EXISTING_ARN"
            echo "â„¹ï¸  WAF Web ACL already exists: $WAF_ARN"
          fi

          # â”€â”€ 2. Store WAF ARN for use in ingress annotation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          echo "WAF_ARN=$WAF_ARN" >> $GITHUB_ENV
          echo "waf_arn=$WAF_ARN" >> $GITHUB_OUTPUT
          echo "âœ… WAF ARN stored: $WAF_ARN"

          # â”€â”€ 3. Enable WAF logging to CloudWatch â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          LOG_GROUP_ARN=$(aws logs describe-log-groups             --log-group-name-prefix "/agentic-trading/eks/application"             --region "$REGION"             --query 'logGroups[0].arn'             --output text | sed 's/:log-group:.*/:log-group:\/agentic-trading\/eks\/application:*/')

          aws wafv2 put-logging-configuration             --logging-configuration "ResourceArn=${WAF_ARN},LogDestinationConfigs=[arn:aws:logs:${REGION}:${AWS_ACCOUNT_ID}:log-group:/agentic-trading/eks/application]"             --region "$REGION" 2>/dev/null || echo "WAF logging config may need manual setup â€” skipping"



  # ============================================================
  # JOB 4 â€” Deploy app to EKS
  # ============================================================
  deploy:
    name: Deploy to EKS
    runs-on: ubuntu-latest
    needs: [ build-and-push, provision-cluster ]
    environment: production

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id:     ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region:            ${{ env.AWS_REGION }}

      - name: Install kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: "v1.29.0"

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --name "$CLUSTER_NAME" --region "$AWS_REGION"

      - name: Fetch WAF ARN
        run: |
          # Prefer pinned secret, fall back to job output, then live lookup
          WAF_ARN="${{ secrets.WAF_ARN }}"
          if [ -z "$WAF_ARN" ] || [ "$WAF_ARN" = "None" ]; then
            WAF_ARN="${{ needs.provision-cluster.outputs.waf_arn }}"
          fi
          if [ -z "$WAF_ARN" ] || [ "$WAF_ARN" = "None" ]; then
            WAF_ARN=$(aws wafv2 list-web-acls \
              --scope REGIONAL \
              --region "$AWS_REGION" \
              --query "WebACLs[?Name=='agentic-trading-waf'].ARN" \
              --output text 2>/dev/null)
          fi
          echo "WAF_ARN=$WAF_ARN" >> $GITHUB_ENV
          echo "âœ… WAF ARN: $WAF_ARN"

      - name: Resolve ACM certificate ARN
        id: acm
        run: |
          # Pinned wildcard cert *.opssightai.com
          # Override by setting ACM_CERT_ARN in GitHub Secrets
          if [ -n "${{ secrets.ACM_CERT_ARN }}" ]; then
            CERT_ARN="${{ secrets.ACM_CERT_ARN }}"
          else
            CERT_ARN="arn:aws:acm:us-east-1:713220200108:certificate/7a263fe4-a8d5-47cc-a361-8b0a85a4c29e"
          fi
          echo "cert_arn=$CERT_ARN" >> $GITHUB_OUTPUT
          echo "Using cert: $CERT_ARN"

      - name: Generate deployment manifest
        env:
          ACM_CERT_ARN: ${{ steps.acm.outputs.cert_arn }}
        run: |
          ECR_BASE="${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com"
          IMAGE_TAG="${{ needs.build-and-push.outputs.image-tag }}"

          cat > /tmp/k8s-deploy.yaml << MANIFEST
          ---
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: agentic-trading-backend
            namespace: ${NAMESPACE}
            labels:
              app: agentic-trading-backend
              version: "${IMAGE_TAG}"
          spec:
            replicas: 2
            selector:
              matchLabels:
                app: agentic-trading-backend
            strategy:
              type: RollingUpdate
              rollingUpdate:
                maxSurge: 1
                maxUnavailable: 0
            template:
              metadata:
                labels:
                  app: agentic-trading-backend
                  version: "${IMAGE_TAG}"
              spec:
                containers:
                - name: backend
                  image: ${ECR_BASE}/${BACKEND_IMAGE}:${IMAGE_TAG}
                  imagePullPolicy: Always
                  ports:
                  - containerPort: 8000
                  resources:
                    requests:
                      cpu: "250m"
                      memory: "512Mi"
                    limits:
                      cpu: "1000m"
                      memory: "1Gi"
                  env:
                  - name: CORS_ALLOWED_ORIGINS
                    value: "https://agentictradepulse.opssightai.com"
                  - name: OPENAI_API_KEY
                    valueFrom:
                      secretKeyRef:
                        name: trading-app-secrets
                        key: openai-api-key
                  - name: ALPACA_API_KEY
                    valueFrom:
                      secretKeyRef:
                        name: trading-app-secrets
                        key: alpaca-api-key
                  - name: ALPACA_SECRET_KEY
                    valueFrom:
                      secretKeyRef:
                        name: trading-app-secrets
                        key: alpaca-secret-key
                  - name: APP_API_KEY
                    valueFrom:
                      secretKeyRef:
                        name: trading-app-secrets
                        key: app-api-key
                  - name: DATABASE_URL
                    valueFrom:
                      secretKeyRef:
                        name: trading-app-secrets
                        key: database-url
                  - name: JWT_SECRET
                    valueFrom:
                      secretKeyRef:
                        name: trading-app-secrets
                        key: jwt-secret
                  - name: ADMIN_USERNAME
                    valueFrom:
                      secretKeyRef:
                        name: trading-app-secrets
                        key: admin-username
                  - name: ADMIN_PASSWORD_HASH
                    valueFrom:
                      secretKeyRef:
                        name: trading-app-secrets
                        key: admin-password-hash
                  - name: ADMIN_TOTP_SECRET
                    valueFrom:
                      secretKeyRef:
                        name: trading-app-secrets
                        key: admin-totp-secret
                        optional: true
                  livenessProbe:
                    httpGet:
                      path: /health
                      port: 8000
                    initialDelaySeconds: 15
                    periodSeconds: 20
                    failureThreshold: 3
                  readinessProbe:
                    httpGet:
                      path: /health
                      port: 8000
                    initialDelaySeconds: 5
                    periodSeconds: 10
                    failureThreshold: 3
          ---
          apiVersion: v1
          kind: Service
          metadata:
            name: agentic-trading-backend
            namespace: ${NAMESPACE}
          spec:
            selector:
              app: agentic-trading-backend
            ports:
            - protocol: TCP
              port: 8000
              targetPort: 8000
            type: ClusterIP
          ---
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: agentic-trading-frontend
            namespace: ${NAMESPACE}
            labels:
              app: agentic-trading-frontend
              version: "${IMAGE_TAG}"
          spec:
            replicas: 2
            selector:
              matchLabels:
                app: agentic-trading-frontend
            strategy:
              type: RollingUpdate
              rollingUpdate:
                maxSurge: 1
                maxUnavailable: 0
            template:
              metadata:
                labels:
                  app: agentic-trading-frontend
                  version: "${IMAGE_TAG}"
              spec:
                containers:
                - name: frontend
                  image: ${ECR_BASE}/${FRONTEND_IMAGE}:${IMAGE_TAG}
                  imagePullPolicy: Always
                  ports:
                  - containerPort: 80
                  resources:
                    requests:
                      cpu: "100m"
                      memory: "128Mi"
                    limits:
                      cpu: "250m"
                      memory: "256Mi"
                  livenessProbe:
                    httpGet:
                      path: /
                      port: 80
                    initialDelaySeconds: 10
                    periodSeconds: 15
                  readinessProbe:
                    httpGet:
                      path: /
                      port: 80
                    initialDelaySeconds: 5
                    periodSeconds: 10
          ---
          apiVersion: v1
          kind: Service
          metadata:
            name: agentic-trading-frontend
            namespace: ${NAMESPACE}
          spec:
            selector:
              app: agentic-trading-frontend
            ports:
            - protocol: TCP
              port: 80
              targetPort: 80
            type: ClusterIP
          ---
          apiVersion: networking.k8s.io/v1
          kind: Ingress
          metadata:
            name: agentic-trading-ingress
            namespace: ${NAMESPACE}
            annotations:
              kubernetes.io/ingress.class: alb
              alb.ingress.kubernetes.io/scheme: internet-facing
              alb.ingress.kubernetes.io/target-type: ip
              alb.ingress.kubernetes.io/healthcheck-path: /health
              alb.ingress.kubernetes.io/healthcheck-interval-seconds: "15"
              alb.ingress.kubernetes.io/success-codes: "200"
              alb.ingress.kubernetes.io/load-balancer-name: agentic-trading-alb
              alb.ingress.kubernetes.io/certificate-arn: ${ACM_CERT_ARN}
              alb.ingress.kubernetes.io/listen-ports: '[{"HTTP":80},{"HTTPS":443}]'
              alb.ingress.kubernetes.io/ssl-redirect: '443'
              alb.ingress.kubernetes.io/ssl-policy: ELBSecurityPolicy-TLS13-1-2-2021-06
              alb.ingress.kubernetes.io/load-balancer-attributes: routing.http.drop_invalid_header_fields.enabled=true
              alb.ingress.kubernetes.io/wafv2-acl-arn: ${WAF_ARN}
          spec:
            rules:
            - host: agentictradepulse.opssightai.com
              http:
                paths:
                - path: /api
                  pathType: Prefix
                  backend:
                    service:
                      name: agentic-trading-backend
                      port:
                        number: 8000
                - path: /health
                  pathType: Prefix
                  backend:
                    service:
                      name: agentic-trading-backend
                      port:
                        number: 8000
                - path: /
                  pathType: Prefix
                  backend:
                    service:
                      name: agentic-trading-frontend
                      port:
                        number: 80
          MANIFEST
          echo "Manifest generated for image tag: $IMAGE_TAG"

      - name: Apply deployment
        run: |
          kubectl apply -f /tmp/k8s-deploy.yaml
          echo "Manifests applied"

      - name: Wait for backend rollout
        run: |
          kubectl rollout status deployment/agentic-trading-backend \
            -n "$NAMESPACE" --timeout=300s

      - name: Wait for frontend rollout
        run: |
          kubectl rollout status deployment/agentic-trading-frontend \
            -n "$NAMESPACE" --timeout=300s

      - name: Wait for ALB and get URL
        id: alb
        run: |
          echo "Waiting for ALB to be provisioned..."
          for i in $(seq 1 40); do
            ALB_URL=$(kubectl get ingress agentic-trading-ingress \
              -n "$NAMESPACE" \
              -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
            if [[ -n "$ALB_URL" ]]; then
              echo "alb_url=$ALB_URL" >> $GITHUB_OUTPUT
              echo "ALB ready: $ALB_URL"
              break
            fi
            echo "Attempt $i/40 â€” waiting 15s..."
            sleep 15
          done

      - name: Health check
        run: |
          ALB_URL="${{ steps.alb.outputs.alb_url }}"
          if [[ -z "$ALB_URL" ]]; then
            echo "ALB URL not yet available â€” check ingress manually"
            exit 0
          fi
          echo "Running health check against http://$ALB_URL/health ..."
          sleep 20
          for i in $(seq 1 10); do
            STATUS=$(curl -s -o /dev/null -w "%{http_code}" \
              --max-time 10 "http://$ALB_URL/health" || echo "000")
            if [[ "$STATUS" == "200" ]]; then
              echo "Health check PASSED (HTTP 200)"
              break
            fi
            echo "Attempt $i â€” HTTP $STATUS, retrying in 15s..."
            sleep 15
          done

      - name: Print deployment summary
        run: |
          ALB_URL="${{ steps.alb.outputs.alb_url }}"
          IMAGE_TAG="${{ needs.build-and-push.outputs.image-tag }}"
          echo ""
          echo "### Deployment Complete âœ…" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Field | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Cluster | $CLUSTER_NAME |" >> $GITHUB_STEP_SUMMARY
          echo "| Namespace | $NAMESPACE |" >> $GITHUB_STEP_SUMMARY
          echo "| Image Tag | $IMAGE_TAG |" >> $GITHUB_STEP_SUMMARY
          echo "| App URL | http://$ALB_URL |" >> $GITHUB_STEP_SUMMARY
          echo "| Health | http://$ALB_URL/health |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          kubectl get pods -n "$NAMESPACE" >> $GITHUB_STEP_SUMMARY || true
          echo ""
          echo "App URL:    http://$ALB_URL"
          echo "Health URL: http://$ALB_URL/health"
          kubectl get pods -n "$NAMESPACE"
          kubectl get ingress -n "$NAMESPACE"
